{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Download and Extract the Stanford Dog Breed Dataset from the URL Below\n\n* When connecting the Kaggle dataset via Object Storage, reading images one by one takes a long time, which slows down model training.\n* To speed up training, download and extract the images directly to the local disk and use them in the model.\n","metadata":{}},{"cell_type":"code","source":"# Download the Stanford Dog Breed dataset\n!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n# Extract the archive directly into the current directory (/kaggle/working)\n!ls; tar -xvf images.tar\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/working/Images'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create a DataFrame of Metadata (Image Absolute Path and Label) Based on Image Directories and Filenames\n\n* Under the `/kaggle/working/Images` directory, there are subdirectories for each dog breed containing the image files.\n* The label values are generated by extracting the subdirectory name immediately above each image file in its absolute path.\n","metadata":{}},{"cell_type":"code","source":"file_path='/kaggle/working/Images/n02099429-curly-coated_retriever/n02099429_841.jpg'\n# Find the first '/' after the 20th character in the path\nstart_pos = file_path.find('/', 20)  \n# Find the last '/' in the path (from the right)\nend_pos = file_path.rfind('/')  \n# Extract the substring between start_pos and end_pos (e.g., folder name)\nimsi_breed = file_path[start_pos+1:end_pos]\n# From the extracted string, take the part after '-' as the actual label value\nbreed = imsi_breed[imsi_breed.find('-')+1:]\nprint(start_pos, end_pos, imsi_breed, breed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os \n\nIMAGE_DIR = '/kaggle/working/Images' \n\ndef make_dogbreed_dataframe(image_dir=IMAGE_DIR):\n    paths = []\n    label_gubuns = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # Some files in the directory may not be images.\n            if '.jpg' in filename:\n                # Assign the absolute file path to the file_path variable.\n                file_path = dirname + '/' + filename\n                paths.append(file_path)\n                # First extraction for label generation:\n                # split by '/' and take the subdirectory name right above the file.\n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos+1:end_pos]\n                # From the extracted string, take the part after '-' as the label value.\n                breed = imsi_breed[imsi_breed.find('-')+1:]\n                #print(start_pos, end_pos, imsi_breed, breed)\n                label_gubuns.append(breed)\n\n    data_df = pd.DataFrame({'path': paths, 'label': label_gubuns})\n    \n    # Map each label to a numeric target value\n    sorted_label = np.sort(data_df['label'].unique())\n    label_mapping = {label: index for index, label in enumerate(sorted_label)}\n    data_df['target'] = data_df['label'].map(label_mapping)\n    # Sort the DataFrame by target and path for consistency\n    data_df = data_df.sort_values(by=['target', 'path'], ascending=True)\n    \n    return data_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\npd.set_option('display.max_colwidth', 200)\n\ndata_df = make_dogbreed_dataframe()\nprint('data_df shape:', data_df.shape)\ndata_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check the Distribution of Individual Dog Breeds\n","metadata":{}},{"cell_type":"code","source":"print(data_df.shape)\n# count by breed\ndata_df[['label', 'target']].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nplt.figure(figsize=(26, 4))\n\nsns.countplot(data=data_df, x='label')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display dog breed image","metadata":{}},{"cell_type":"code","source":"import cv2\n\ndef show_grid_images(image_path_list, ncols=8, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        axs[i].imshow(image)\n        axs[i].set_title(title)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='American_Staffordshire_terrier']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_02, ncols=6, title='American_Staffordshire_terrier')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_list = data_df['label'].value_counts().index.tolist()\n\nfor iter_cnt, breed in enumerate(breed_list):\n    breed_image_list = data_df[data_df['label']==breed]['path'].iloc[:6].tolist()\n    show_grid_images(breed_image_list, ncols=6, title=breed)\n    if iter_cnt == 4:\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize Images with Applied Augmentation\n","metadata":{}},{"cell_type":"code","source":"import albumentations as A\n\n# Be cautious when using crop. Since both humans and dogs appear in the dataset,\n# if a person is centered, the crop might cut out only the person.\nimsi_augmentor = A.Compose([\n    A.Resize(height=224, width=224, p=1), \n    A.CenterCrop(height=200, width=200, p=1),  # A.CenterCrop(height=180, width=180, p=1)\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5)\n])\n\n# To visualize augmented images, apply the image transformation pipeline \n# defined with Albumentations (augmentor).\n# Resize the images to 224x224.\ndef show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='original Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=imsi_augmentor, ncols=6, title='augmented Staffordshire_bullterrier')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split the Entire DataFrame into Training and Testing DataFrames, Then Further Split the Training DataFrame into Training and Validation Sets\n\n* Use `train_test_split()` to allocate 40% of the total data to the test set.\n* Set the `stratify` parameter to ensure an even distribution of breed labels across the splits.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(data_df, test_size=0.4, stratify=data_df['label'], random_state=2025)\nprint(train_df.shape, test_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df['label'].value_counts()/train_df.shape[0])\nprint(test_df['label'].value_counts()/test_df.shape[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['target'], random_state=2025)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\n\nclass BreedDataset(Dataset):\n    def __init__(self, image_paths, targets=None, transform=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n        \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image_np = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB).astype(np.float32)\n        image = self.transform(image=image_np)['image']\n\n        if self.targets is not None:\n            target = torch.tensor(self.targets[idx])\n            return image, target\n        else:\n            return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nBATCH_SIZE = 16\nclass CFG:\n    batch_size = 16\n    image_size = 224\n\n# Data Augmentation\ntr_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ndef create_tr_val_loader(tr_df, val_df, tr_transform, val_transform):\n    tr_dataset =BreedDataset(image_paths=tr_df['path'].to_list(), \n                               targets=tr_df['target'].to_list(), transform=tr_transform)\n    val_dataset = BreedDataset(image_paths=val_df['path'].to_list(), \n                               targets=val_df['target'].to_list(), transform=val_transform)\n    \n    tr_loader = DataLoader(tr_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=4*BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n    return tr_loader, val_loader\n\ntr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n                                             tr_transform=tr_transform, val_transform=val_transform)\nimages, labels = next(iter(tr_loader))\nprint(images.shape, labels.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create torchvision Models\n\n* Implement a model creation function that allows testing with both ResNet101 and the EfficientNet family.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\ndef create_tv_model(model_name, num_classes=1000):\n    model = None\n    if model_name == 'efficientnet_v2_s':\n        model = models.efficientnet_v2_s(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1280, out_features=num_classes))\n    elif model_name == 'efficientnet_b4':\n        model = models.efficientnet_b4(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1792, out_features=num_classes))\n    elif model_name == 'efficientnet_b1':\n        model = models.efficientnet_b1(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1280, out_features=num_classes))\n    elif model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1280, out_features=num_classes))\n    elif model_name == 'resnet101':\n        model = models.resnet101(weights='DEFAULT')\n        model.fc = nn.Linear(in_features=2048, out_features=num_classes)\n        \n    return model\n\neff_model = create_tv_model('efficientnet_b0', num_classes=120)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import models\n\nmodels.resnet101(weights=None)#resnet101, efficientnet_b0, efficientnet_v2_s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models.ResNet101_Weights.IMAGENET1K_V2.transforms() # EffcientNet_B0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train Models with the Trainer\n\n* ResNet101 and EfficientNet B0/B1\n","metadata":{}},{"cell_type":"code","source":"!rm -rf ./modular/v1\n!mkdir -p ./modular/v1\n!wget -O ./modular/v1/utils.py https://raw.githubusercontent.com/gayoung-k/cnn-learning-notes/main/utils.py\n!ls ./modular/v1\n\nimport sys\n\nsys.path.append('/kaggle/working')\n\nfrom modular.v1.utils import Trainer, ModelCheckpoint, EarlyStopping","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 224\n\n# Horizontal_flip\ntr_transform_01 = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ndef train_breed(model_name, tr_transform, val_transform, learning_rate=1e-3, callbacks=None, epochs=30):\n    tr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n                                             tr_transform=tr_transform, val_transform=val_transform)\n    model = create_tv_model(model_name, num_classes=120)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n    loss_fn = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer=optimizer, mode='min', factor=0.2, patience=3, threshold=0.01, min_lr=1e-7)\n    \n    trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n                   train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, \n                   callbacks=callbacks, device=device)\n    history = trainer.fit(epochs)\n    \n    return trainer, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model: ResNet101, apply only HorizontalFlip as augmentation.  \n# Learning rate: 1e-4, callbacks: None, epochs: 30.  \ntrainer, history = train_breed('resnet101', tr_transform_01, val_transform, learning_rate=1e-4, \n                               callbacks=None, epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from modular.v1.utils import Predictor\n\ntest_image_paths = test_df['path'].to_list()\ntest_targets = test_df['target'].to_list()\n\n#CFG.batch_size = 16\nCFG.image_size = 224\n\ntest_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_dataset = BreedDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision import transforms\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimg_dir=\"/kaggle/input/test-dog-data\"\nimg_names = [\"test_dog.jpg\", \"test_dog2.jpg\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_single_image(image_path, predictor, transform, class_names):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    transformed = transform(image=image)\n    input_tensor = trnasformed['image'].unsqueeze(0).to(predict.device)\n    \n    predictor.model.eval()\n    with torch.no_grad():\n        output_predictor.model(input_tensor)\n        pred_class = output.argmax(dim=1).item()\n    return class_names[pred_class]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Check Which Breeds Were Most Frequently Misclassified\n\n* Compare the actual ground truth breeds with the predicted breeds to see how they were misclassified.\n","metadata":{}},{"cell_type":"code","source":"# Perform predictions on all test data and return results as a NumPy array\ndef get_all_predictions(predictor, test_loader):\n    preds_all_list = []\n    targets_all_list = []\n    # (64, 3, 244, 244) -> (64,)\n    for images, targets in test_loader:\n        preds = predictor.predict(images).cpu().numpy()\n        # Use extend() instead of append() to store individual element values in the list\n        preds_all_list.extend(preds)\n        # targets_all_list.extend(targets.cpu().numpy())\n\n    preds_all = np.array(preds_all_list)\n\n    return preds_all\n\npreds_all = get_all_predictions(predictor, test_loader)\n\n# Save the prediction results into a separate column of test_df\ntest_df['resnet101_pred'] = preds_all\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df[test_df['target'] != test_df['resnet101_pred']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df[test_df['target'] != test_df['resnet101_pred']]['label'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df[test_df['label'] == 'Eskimo_dog'][['target', 'resnet101_pred']].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (224, 224))\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label']=='Siberian_husky']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='Eskimo_dog']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Siberian_husky')\nshow_grid_images(breed_image_list_02, ncols=6, title='Eskimo_dog')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nwrong_result_df = test_df[test_df['target'] != test_df['resnet101_pred']]\n\nplt.figure(figsize=(26, 4))\nplt.xticks(rotation=90)\nsns.countplot(data=wrong_result_df, x='label')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train and Evaluate with EfficientNet-B0\n\n* Train and evaluate the EfficientNet-B0 model using the same batch size, image size, and augmentation\n","metadata":{}},{"cell_type":"code","source":"models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms() # EffcientNet_B0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\n# Train and evaluate the EfficientNet-B0 model using the same batch size, image size, and augmentation\ntrainer, history = train_breed('efficientnet_b0', tr_transform_01, val_transform, learning_rate=1e-4, \n                               callbacks=None, epochs=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train and Evaluate with EfficientNet-B1\n\n* Set the image size to 240\n","metadata":{}},{"cell_type":"code","source":"models.EfficientNet_B1_Weights.IMAGENET1K_V2.transforms() # EffcientNet_B0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 240\n\n# Horizontal_flip\ntr_transform_01 = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntrainer, history = train_breed('efficientnet_b1', tr_transform_01, val_transform, learning_rate=1e-4, \n                               callbacks=None, epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from modular.v1.utils import Predictor\n\nCFG.image_size = 240\n\ntest_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_dataset = BreedDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train and Evaluate the Model with Various Augmentations\n\n* Applying heavy augmentations does not necessarily improve performance (it may even degrade it).\n","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 240\n\n# Horizontal_flip\nfrom torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 240\n\ntr_transform_02 = A.Compose([\n    A.RandomResizedCrop(height=180, width=180, scale=(0.5, 1.0), p=0.3),\n    A.HorizontalFlip(p=0.3),\n    A.VerticalFlip(p=0.3),\n    A.ShiftScaleRotate(p=0.2),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2),\n    A.OneOf(\n        [A.CoarseDropout(p=1, max_holes=26), \n         A.CLAHE(p=1)\n        ], p=0.3), \n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=tr_transform_02, ncols=6, title='augmented')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer, history = train_breed('efficientnet_b1', tr_transform_02, val_transform, learning_rate=1e-4, epochs=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from modular.v1.utils import Predictor\n\ntest_image_paths = test_df['path'].to_list()\ntest_targets = test_df['target'].to_list()\n\nCFG.image_size = 240\n\ntest_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_dataset = BreedDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train and Evaluate the Model After Applying Different Augmentations\n","metadata":{}},{"cell_type":"code","source":"tr_transform_03 = A.Compose([\n    A.HorizontalFlip(p=0.3),\n    A.ShiftScaleRotate(scale_limit=(0.7, 0.9), p=0.2, rotate_limit=30),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2),\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=tr_transform_03, ncols=6, title='augmented')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer, history = train_breed('efficientnet_b1', tr_transform_03, val_transform, learning_rate=1e-4, epochs=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}