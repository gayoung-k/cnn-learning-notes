{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8367c2ee",
   "metadata": {
    "papermill": {
     "duration": 0.005187,
     "end_time": "2025-08-31T20:13:48.688723",
     "exception": false,
     "start_time": "2025-08-31T20:13:48.683536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Using models module in torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a25d9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:13:48.698797Z",
     "iopub.status.busy": "2025-08-31T20:13:48.698051Z",
     "iopub.status.idle": "2025-08-31T20:13:58.514580Z",
     "shell.execute_reply": "2025-08-31T20:13:58.513793Z"
    },
    "papermill": {
     "duration": 9.823169,
     "end_time": "2025-08-31T20:13:58.516116",
     "exception": false,
     "start_time": "2025-08-31T20:13:48.692947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 211MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def create_resnet_model(model_name, num_classes=10, weights=None):\n",
    "    model = None\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(weights=weights)\n",
    "    elif model_name == 'resnext50_32x4d':\n",
    "        model = models.resnext50_32x4d(weights=weights)\n",
    "    \n",
    "    num_in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features=num_in_features, out_features=num_classes)\n",
    "\n",
    "    return model\n",
    "    \n",
    "model = create_resnet_model('resnet50', num_classes=2, weights='DEFAULT') #resnet50, resnext50_32x4d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfd258",
   "metadata": {
    "papermill": {
     "duration": 0.004206,
     "end_time": "2025-08-31T20:13:58.525316",
     "exception": false,
     "start_time": "2025-08-31T20:13:58.521110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Download cat and dog dataset and create meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5079622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:13:58.535089Z",
     "iopub.status.busy": "2025-08-31T20:13:58.534783Z",
     "iopub.status.idle": "2025-08-31T20:14:01.149166Z",
     "shell.execute_reply": "2025-08-31T20:14:01.148522Z"
    },
    "papermill": {
     "duration": 2.62098,
     "end_time": "2025-08-31T20:14:01.150510",
     "exception": false,
     "start_time": "2025-08-31T20:13:58.529530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_cnd_meta_df(file_dir):\n",
    "    paths = []  # list of image file paths\n",
    "    dataset_gubuns = []  # train/test split\n",
    "    label_gubuns = []  # dog and cat\n",
    "    # Use os.walk() to traverse all subdirectories under a specific directory.\n",
    "    # Under the cat-and-dog directory, all files with the jpg extension are image files.\n",
    "    # The cat-and-dog directory contains /train/ and /test/ subdirectories (holding training and test images).\n",
    "    for dirname, _, filenames in os.walk(file_dir):\n",
    "        for filename in filenames:\n",
    "            # Non-image files may also exist in this directory.\n",
    "            if '.jpg' in filename:\n",
    "                # Assign the absolute path of the file to the file_path variable.\n",
    "                file_path = dirname + '/' + filename\n",
    "                paths.append(file_path)\n",
    "                # If the absolute path contains training_set or test_set, classify dataset as 'train' or 'test'.\n",
    "                if '/training_set/' in file_path:\n",
    "                    dataset_gubuns.append('train')  \n",
    "                elif '/test_set/' in file_path:\n",
    "                    dataset_gubuns.append('test')\n",
    "                else: \n",
    "                    dataset_gubuns.append('N/A')\n",
    "                \n",
    "                # If the absolute path contains dogs, the file is a dog image; if it contains cats, it's a cat image.\n",
    "                if 'dogs' in file_path:\n",
    "                    label_gubuns.append('DOG')\n",
    "                elif 'cats' in file_path:\n",
    "                    label_gubuns.append('CAT')\n",
    "                else: \n",
    "                    label_gubuns.append('N/A')\n",
    "    # Create a DataFrame containing metadata.\n",
    "    data_df = pd.DataFrame({'path': paths, \n",
    "                            'dataset': dataset_gubuns, \n",
    "                            'label': label_gubuns})\n",
    "    # Convert target values to 0 and 1\n",
    "    label_mapping = {'DOG': 0, 'CAT': 1}\n",
    "    data_df['target'] = data_df['label'].map(label_mapping)\n",
    "\n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d86e6",
   "metadata": {
    "papermill": {
     "duration": 0.004617,
     "end_time": "2025-08-31T20:14:01.159875",
     "exception": false,
     "start_time": "2025-08-31T20:14:01.155258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42ababd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:14:01.170413Z",
     "iopub.status.busy": "2025-08-31T20:14:01.170071Z",
     "iopub.status.idle": "2025-08-31T20:14:14.934367Z",
     "shell.execute_reply": "2025-08-31T20:14:14.933405Z"
    },
    "papermill": {
     "duration": 13.771316,
     "end_time": "2025-08-31T20:14:14.935649",
     "exception": false,
     "start_time": "2025-08-31T20:14:01.164333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10028, 4)\n"
     ]
    }
   ],
   "source": [
    "data_df = create_cnd_meta_df(file_dir='/kaggle/input/')\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b38803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:14:14.946099Z",
     "iopub.status.busy": "2025-08-31T20:14:14.945877Z",
     "iopub.status.idle": "2025-08-31T20:14:14.961864Z",
     "shell.execute_reply": "2025-08-31T20:14:14.961189Z"
    },
    "papermill": {
     "duration": 0.022234,
     "end_time": "2025-08-31T20:14:14.962912",
     "exception": false,
     "start_time": "2025-08-31T20:14:14.940678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10028, 4) (8005, 4) (2001, 4) (2001, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = data_df[data_df['dataset']=='train']\n",
    "test_df = data_df[data_df['dataset']=='test']\n",
    "\n",
    "train_df_temp, _ = train_test_split(train_df, test_size=0.5, stratify=train_df['target'], random_state=2025)\n",
    "\n",
    "tr_df, val_df = train_test_split(train_df_temp, test_size=0.5, stratify=train_df_temp['target'], random_state=2025)\n",
    "\n",
    "print(data_df.shape, train_df.shape, tr_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5237cf72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:14:14.973010Z",
     "iopub.status.busy": "2025-08-31T20:14:14.972559Z",
     "iopub.status.idle": "2025-08-31T20:14:14.977730Z",
     "shell.execute_reply": "2025-08-31T20:14:14.977182Z"
    },
    "papermill": {
     "duration": 0.011372,
     "end_time": "2025-08-31T20:14:14.978766",
     "exception": false,
     "start_time": "2025-08-31T20:14:14.967394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CnD_Dataset(Dataset):\n",
    "    # Takes in image file list, target list, transforms, and other arguments needed for processing images and targets\n",
    "    def __init__(self, image_paths, targets=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    # Returns the total number of items\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    # Returns a single image and label as tensors based on the specified index\n",
    "    def __getitem__(self, idx):    \n",
    "        # Load the image using PIL and return a PIL Image object\n",
    "        pil_image = Image.open(self.image_paths[idx])\n",
    "        # Typically transform is not None (at least Tensor conversion is applied)\n",
    "        image = self.transform(pil_image)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            # Convert the individual target value to a tensor\n",
    "            target = torch.tensor(self.targets[idx])\n",
    "            return image, target\n",
    "        # For test data, targets may not be provided, so handle this case\n",
    "        else:\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfdca52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:14:14.988371Z",
     "iopub.status.busy": "2025-08-31T20:14:14.988180Z",
     "iopub.status.idle": "2025-08-31T20:14:14.994245Z",
     "shell.execute_reply": "2025-08-31T20:14:14.993761Z"
    },
    "papermill": {
     "duration": 0.011968,
     "end_time": "2025-08-31T20:14:14.995194",
     "exception": false,
     "start_time": "2025-08-31T20:14:14.983226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 224\n",
    "IMG_MEANS = [0.485, 0.456, 0.406] \n",
    "IMG_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def create_tr_val_loader(tr_df, val_df, transform):\n",
    "    tr_dataset = CnD_Dataset(image_paths=tr_df['path'].to_list(), \n",
    "                            targets=tr_df['target'].to_list(), transform=transform)\n",
    "    val_dataset = CnD_Dataset(image_paths=val_df['path'].to_list(), \n",
    "                            targets=val_df['target'].to_list(), transform=transform)\n",
    "    tr_loader = DataLoader(tr_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2*BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return tr_loader, val_loader\n",
    "\n",
    "transform_01 = T.Compose([\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "tr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, transform=transform_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346ceba",
   "metadata": {
    "papermill": {
     "duration": 0.004295,
     "end_time": "2025-08-31T20:14:15.004179",
     "exception": false,
     "start_time": "2025-08-31T20:14:14.999884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainer Class Creation – Apply ModelCheckpoint and Early Stopping Logic\n",
    "\n",
    "* Add `checkpoint_dir` and `early_patience` arguments to the constructor.\n",
    "* Apply ModelCheckpoint and Early Stopping logic to the `fit()` method.\n",
    "* In PyTorch, saving a model:\n",
    "\n",
    "  * `torch.save(model, 'filename.pt')`: Saves both the model architecture (including optimizer) and parameters (weights).\n",
    "  * `torch.save(model.state_dict(), 'model_state_filename.pt')`: Saves only the parameters. More commonly used.\n",
    "* Loading a PyTorch model:\n",
    "\n",
    "  * `model = torch.load('filename.pt')`: Loads the entire model (architecture and parameters together).\n",
    "  * `model.load_state_dict(torch.load('model_state_filename.pt'))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3427c83b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:14:15.014219Z",
     "iopub.status.busy": "2025-08-31T20:14:15.014013Z",
     "iopub.status.idle": "2025-08-31T20:14:15.030486Z",
     "shell.execute_reply": "2025-08-31T20:14:15.029811Z"
    },
    "papermill": {
     "duration": 0.022935,
     "end_time": "2025-08-31T20:14:15.031605",
     "exception": false,
     "start_time": "2025-08-31T20:14:15.008670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer_01:\n",
    "    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, scheduler=None, \n",
    "                 checkpoint_dir='/kaggle/working/checkpoints', early_patience=5, device=None):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        # add scheduler\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        # add current learning rate variable\n",
    "        self.current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        # set variables for checkpoint and early stopping\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.early_patience = early_patience\n",
    "        # initialize best loss as infinity since smaller loss is better\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "\n",
    "        # calculate running average loss\n",
    "        accu_loss = 0.0\n",
    "        running_avg_loss = 0.0\n",
    "        # accuracy, total count and accumulated correct count\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        # visualize training loop progress with tqdm\n",
    "        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "                # must use to(self.device), not to(device)\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                # forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # accumulate loss for each batch and calculate running average\n",
    "                accu_loss += loss.item()\n",
    "                running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                # calculate accuracy metric\n",
    "                # count matches between predicted class values and targets\n",
    "                num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                # calculate accuracy using total accumulated samples and correct predictions\n",
    "                num_total += inputs.shape[0]\n",
    "                accu_num_correct += num_correct\n",
    "                accuracy = accu_num_correct / num_total\n",
    "\n",
    "                # update tqdm progress bar with running average loss and accuracy\n",
    "                progress_bar.update(1)\n",
    "                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # update every 20 batches or last batch\n",
    "                    progress_bar.set_postfix({\"Loss\": running_avg_loss,\n",
    "                                              \"Accuracy\": accuracy})\n",
    "\n",
    "        if (self.scheduler is not None) and (not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)):\n",
    "            self.scheduler.step()\n",
    "            self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "        \n",
    "        return running_avg_loss, accuracy\n",
    "\n",
    "    def validate_epoch(self, epoch):\n",
    "        if not self.val_loader:\n",
    "            return None\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # calculate running average loss\n",
    "        accu_loss = 0\n",
    "        running_avg_loss = 0\n",
    "        # accuracy, total count and accumulated correct count\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "\n",
    "                    outputs = self.model(inputs)\n",
    "\n",
    "                    loss = self.loss_fn(outputs, targets)\n",
    "                    # accumulate loss for each batch and calculate running average\n",
    "                    accu_loss += loss.item()\n",
    "                    running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                    # calculate accuracy metric\n",
    "                    # count matches between predicted class values and targets\n",
    "                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                    # calculate accuracy using total accumulated samples and correct predictions\n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    accuracy = accu_num_correct / num_total\n",
    "                    \n",
    "                    # update tqdm progress bar with running average loss and accuracy\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 40 == 0 or (batch_idx + 1) == progress_bar.total:  # update every 40 batches or last batch\n",
    "                        progress_bar.set_postfix({\"Loss\": running_avg_loss,\n",
    "                                                  \"Accuracy\":accuracy})\n",
    "        # pass epoch-level validation loss to scheduler\n",
    "        if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            self.scheduler.step(running_avg_loss)\n",
    "            self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "\n",
    "        return running_avg_loss, accuracy\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        # create history dict to record train/validation results per epoch, including learning rate\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate_epoch(epoch)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n",
    "                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\",\n",
    "                  f\", Current lr:{self.current_lr:.6f}\")\n",
    "            # record train/validation results per epoch, including learning rate\n",
    "            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(self.current_lr)\n",
    "\n",
    "            # if current loss improves compared to best_val_loss\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss=val_loss\n",
    "                # save model weights\n",
    "                self.save_checkpoint(epoch, val_loss)\n",
    "                # reset early stopping counter\n",
    "                self.early_stopping_counter = 0\n",
    "            else:\n",
    "                self. early_stopping_counter += 1\n",
    "                if(self.early_stopping_counter >= self.early_patience):\n",
    "                    print(\"Early Stopping happens and training stops\")\n",
    "                    break\n",
    "                                \n",
    "        return history\n",
    "\n",
    "    # save model to file when performance improves compared to previous epoch\n",
    "    def save_checkpoint(self, epoch, val_loss):\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch+1}_loss_{val_loss:.4f}.pt')\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved model checkpoint at {checkpoint_path}\")\n",
    "\n",
    "    # return the trained model\n",
    "    def get_trained_model(self):\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040472d8",
   "metadata": {
    "papermill": {
     "duration": 0.004361,
     "end_time": "2025-08-31T20:14:15.040649",
     "exception": false,
     "start_time": "2025-08-31T20:14:15.036288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Train using the modified Trainer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c877ac12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:14:15.050141Z",
     "iopub.status.busy": "2025-08-31T20:14:15.049924Z",
     "iopub.status.idle": "2025-08-31T20:16:28.375003Z",
     "shell.execute_reply": "2025-08-31T20:16:28.374067Z"
    },
    "papermill": {
     "duration": 133.331265,
     "end_time": "2025-08-31T20:16:28.376259",
     "exception": false,
     "start_time": "2025-08-31T20:14:15.044994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Training..]: 100%|██████████| 126/126 [00:13<00:00,  9.52it/s, Loss=0.332, Accuracy=0.873]\n",
      "Epoch 1 [Validating]: 100%|██████████| 63/63 [00:05<00:00, 11.23it/s, Loss=0.48, Accuracy=0.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.3315 Train Accuracy: 0.8726 , Val Loss: 0.4799 Val Accuracy: 0.8456 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_1_loss_0.4799.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.202, Accuracy=0.92]\n",
      "Epoch 2 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.70it/s, Loss=0.826, Accuracy=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 0.2020 Train Accuracy: 0.9195 , Val Loss: 0.8264 Val Accuracy: 0.8116 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.59it/s, Loss=0.173, Accuracy=0.94]\n",
      "Epoch 3 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.93it/s, Loss=0.278, Accuracy=0.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.1726 Train Accuracy: 0.9395 , Val Loss: 0.2779 Val Accuracy: 0.8996 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_3_loss_0.2779.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.187, Accuracy=0.929]\n",
      "Epoch 4 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.59it/s, Loss=0.258, Accuracy=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.1870 Train Accuracy: 0.9285 , Val Loss: 0.2582 Val Accuracy: 0.9055 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_4_loss_0.2582.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.214, Accuracy=0.919]\n",
      "Epoch 5 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 15.03it/s, Loss=0.192, Accuracy=0.922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.2143 Train Accuracy: 0.9185 , Val Loss: 0.1924 Val Accuracy: 0.9220 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_5_loss_0.1924.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.153, Accuracy=0.948]\n",
      "Epoch 6 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.95it/s, Loss=0.193, Accuracy=0.919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.1534 Train Accuracy: 0.9480 , Val Loss: 0.1928 Val Accuracy: 0.9185 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.61it/s, Loss=0.186, Accuracy=0.931]\n",
      "Epoch 7 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s, Loss=0.259, Accuracy=0.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.1858 Train Accuracy: 0.9305 , Val Loss: 0.2587 Val Accuracy: 0.8966 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.60it/s, Loss=0.0891, Accuracy=0.972]\n",
      "Epoch 8 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 15.00it/s, Loss=0.197, Accuracy=0.932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 0.0891 Train Accuracy: 0.9715 , Val Loss: 0.1970 Val Accuracy: 0.9320 , Current lr:0.001000\n",
      "Early Stopping happens and training stops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = create_resnet_model('resnet50', num_classes=2, weights='DEFAULT')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=0.001) # model.fc.parameters(), weight_decay=0.9\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Increase patience to 5.\n",
    "scheduler = ReduceLROnPlateau(\n",
    "            optimizer=optimizer, mode='min', factor=0.5, patience=5, threshold=0.01, min_lr=0.00001)\n",
    "\n",
    "# Input the directory where checkpoint files will be saved and the early-stop patience value.\n",
    "trainer = Trainer_01(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                  train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, \n",
    "                  checkpoint_dir='checkpoints', early_patience=3,\n",
    "                  device=device)\n",
    "# Training and evaluation\n",
    "history = trainer.fit(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a111ba",
   "metadata": {
    "papermill": {
     "duration": 0.085865,
     "end_time": "2025-08-31T20:16:28.500104",
     "exception": false,
     "start_time": "2025-08-31T20:16:28.414239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7892aed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:28.576502Z",
     "iopub.status.busy": "2025-08-31T20:16:28.575965Z",
     "iopub.status.idle": "2025-08-31T20:16:28.583981Z",
     "shell.execute_reply": "2025-08-31T20:16:28.583325Z"
    },
    "papermill": {
     "duration": 0.047825,
     "end_time": "2025-08-31T20:16:28.585085",
     "exception": false,
     "start_time": "2025-08-31T20:16:28.537260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        eval_metric = 0.0\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "\n",
    "        with tqdm(total=len(loader), desc=f\"[Evaluating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "                    pred = self.model(inputs)\n",
    "\n",
    "                    num_correct = (pred.argmax(-1) == targets).sum().item()\n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    eval_metric = accu_num_correct / num_total\n",
    "\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:\n",
    "                        progress_bar.set_postfix({\"Accuracy\": eval_metric})\n",
    "        \n",
    "        return eval_metric\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            pred_proba = F.softmax(outputs, dim=-1) \n",
    "        return pred_proba\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        pred_proba = self.predict_proba(inputs)\n",
    "        pred_class = torch.argmax(pred_proba, dim=-1)\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed857405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:28.662721Z",
     "iopub.status.busy": "2025-08-31T20:16:28.662027Z",
     "iopub.status.idle": "2025-08-31T20:16:34.245565Z",
     "shell.execute_reply": "2025-08-31T20:16:34.244638Z"
    },
    "papermill": {
     "duration": 5.624676,
     "end_time": "2025-08-31T20:16:34.246959",
     "exception": false,
     "start_time": "2025-08-31T20:16:28.622283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Evaluating]: 100%|██████████| 64/64 [00:05<00:00, 11.49it/s, Accuracy=0.936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset evaluation:0.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_image_paths = test_df['path'].to_list()\n",
    "test_targets = test_df['target'].to_list()\n",
    "\n",
    "IMG_SIZE=224\n",
    "test_transform = T.Compose([\n",
    "                        T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "                        T.ToTensor(), \n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = CnD_Dataset(image_paths=test_image_paths, \n",
    "                            targets=test_targets, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa28a0",
   "metadata": {
    "papermill": {
     "duration": 0.038103,
     "end_time": "2025-08-31T20:16:34.325304",
     "exception": false,
     "start_time": "2025-08-31T20:16:34.287201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Checkpoint File Verification\n",
    "\n",
    "* Check the files generated in the `/kaggle/working/checkpoints` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c14762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:34.403667Z",
     "iopub.status.busy": "2025-08-31T20:16:34.403061Z",
     "iopub.status.idle": "2025-08-31T20:16:34.545820Z",
     "shell.execute_reply": "2025-08-31T20:16:34.545035Z"
    },
    "papermill": {
     "duration": 0.183777,
     "end_time": "2025-08-31T20:16:34.547195",
     "exception": false,
     "start_time": "2025-08-31T20:16:34.363418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_epoch_1_loss_0.4799.pt  checkpoint_epoch_4_loss_0.2582.pt\r\n",
      "checkpoint_epoch_3_loss_0.2779.pt  checkpoint_epoch_5_loss_0.1924.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e7623a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:34.626615Z",
     "iopub.status.busy": "2025-08-31T20:16:34.626326Z",
     "iopub.status.idle": "2025-08-31T20:16:35.138977Z",
     "shell.execute_reply": "2025-08-31T20:16:35.138335Z"
    },
    "papermill": {
     "duration": 0.553602,
     "end_time": "2025-08-31T20:16:35.140151",
     "exception": false,
     "start_time": "2025-08-31T20:16:34.586549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /kaggle/working/checkpoints/checkpoint_epoch_5_loss_0.1924.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "\n",
    "ckpt_dir = \"/kaggle/working/checkpoints\"\n",
    "\n",
    "# Get all checkpoint files from the directory\n",
    "checkpoint_files = glob.glob(os.path.join(ckpt_dir, \"checkpoint_epoch_*.pt\"))\n",
    "\n",
    "# Function to extract the epoch number from the filename\n",
    "def extract_epoch_num(filename):\n",
    "    match = re.search(r\"epoch_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Select the checkpoint with the highest epoch number\n",
    "latest_ckpt = max(checkpoint_files, key=extract_epoch_num)\n",
    "print(\"Loading checkpoint:\", latest_ckpt)\n",
    "\n",
    "# Load state_dict from the latest checkpoint\n",
    "state_dict = torch.load(latest_ckpt, weights_only=True)\n",
    "\n",
    "# Create a model without pretrained weights\n",
    "new_model = create_resnet_model('resnet50', num_classes=2, weights=None)\n",
    "\n",
    "# Load the parameters from checkpoint into the model\n",
    "new_model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4932962c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:35.219150Z",
     "iopub.status.busy": "2025-08-31T20:16:35.218915Z",
     "iopub.status.idle": "2025-08-31T20:16:39.551931Z",
     "shell.execute_reply": "2025-08-31T20:16:39.550797Z"
    },
    "papermill": {
     "duration": 4.373517,
     "end_time": "2025-08-31T20:16:39.553224",
     "exception": false,
     "start_time": "2025-08-31T20:16:35.179707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Evaluating]: 100%|██████████| 64/64 [00:04<00:00, 14.91it/s, Accuracy=0.913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset evaluation:0.9135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model=new_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d9c08",
   "metadata": {
    "papermill": {
     "duration": 0.04079,
     "end_time": "2025-08-31T20:16:39.635193",
     "exception": false,
     "start_time": "2025-08-31T20:16:39.594403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create ModelCheckPoint class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b8389bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:39.718932Z",
     "iopub.status.busy": "2025-08-31T20:16:39.718582Z",
     "iopub.status.idle": "2025-08-31T20:16:39.726255Z",
     "shell.execute_reply": "2025-08-31T20:16:39.725661Z"
    },
    "papermill": {
     "duration": 0.05018,
     "end_time": "2025-08-31T20:16:39.727301",
     "exception": false,
     "start_time": "2025-08-31T20:16:39.677121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, checkpoint_dir='checkpoints', monitor='val_loss', mode='min', save_interval=1, verbose=1):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_value = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.verbose = verbose\n",
    "        self.save_interval = save_interval\n",
    "        self._make_checkpoint_dir_unless()\n",
    "\n",
    "    def _make_checkpoint_dir_unless(self):\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "    \n",
    "    # Depending on the mode, check if the metric value has improved compared to the previous epoch and return True/False\n",
    "    def is_improvement(self, value):\n",
    "        if self.mode == 'min':\n",
    "            return value < self.best_value\n",
    "        else:\n",
    "            return value > self.best_value\n",
    "\n",
    "    # Update self.best_value, performed only if is_improvement() returns True\n",
    "    def update_best_value(self, value):\n",
    "        self.best_value = value\n",
    "\n",
    "    def save(self, model, epoch, value):\n",
    "        if self.save_interval == 1:\n",
    "            if self.is_improvement(value):\n",
    "                self._checkpoint_save(model, epoch, value)\n",
    "                self.update_best_value(value)\n",
    "            \n",
    "        elif self.save_interval > 1:\n",
    "            if (epoch + 1) % self.save_interval == 0:\n",
    "                self._checkpoint_save(model, epoch, value)\n",
    "            \n",
    "    def _checkpoint_save(self, model, epoch, value):\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, \n",
    "                                       f'checkpoint_epoch_{epoch+1}_{self.monitor}_{value:.4f}.pt')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        if self.verbose:\n",
    "            print(f\"Saved model checkpoint at {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da14be4",
   "metadata": {
    "papermill": {
     "duration": 0.040491,
     "end_time": "2025-08-31T20:16:39.807871",
     "exception": false,
     "start_time": "2025-08-31T20:16:39.767380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Modify the Trainer class to apply the ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af9edfe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:39.890350Z",
     "iopub.status.busy": "2025-08-31T20:16:39.889820Z",
     "iopub.status.idle": "2025-08-31T20:16:39.905960Z",
     "shell.execute_reply": "2025-08-31T20:16:39.905384Z"
    },
    "papermill": {
     "duration": 0.058885,
     "end_time": "2025-08-31T20:16:39.907103",
     "exception": false,
     "start_time": "2025-08-31T20:16:39.848218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer_02:\n",
    "    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, scheduler=None, \n",
    "                 checkpoint_cb=None, device=None):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        self.checkpoint_cb = checkpoint_cb\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "\n",
    "        accu_loss = 0.0\n",
    "        running_avg_loss = 0.0\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "                # 반드시 to(self.device). to(device) 아님.\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                accu_loss += loss.item()\n",
    "                running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                num_total += inputs.shape[0]\n",
    "                accu_num_correct += num_correct\n",
    "                accuracy = accu_num_correct / num_total\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  \n",
    "                    progress_bar.set_postfix({\"Loss\": running_avg_loss,\n",
    "                                              \"Accuracy\": accuracy})\n",
    "\n",
    "        if (self.scheduler is not None) and (not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)):\n",
    "            self.scheduler.step()\n",
    "            self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "        \n",
    "        return running_avg_loss, accuracy\n",
    "\n",
    "    def validate_epoch(self, epoch):\n",
    "        if not self.val_loader:\n",
    "            return None\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        accu_loss = 0\n",
    "        running_avg_loss = 0\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "\n",
    "                    outputs = self.model(inputs)\n",
    "\n",
    "                    loss = self.loss_fn(outputs, targets)\n",
    "                    accu_loss += loss.item()\n",
    "                    running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    accuracy = accu_num_correct / num_total\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  \n",
    "                        progress_bar.set_postfix({\"Loss\": running_avg_loss,\n",
    "                                                  \"Accuracy\":accuracy})\n",
    "        # scheduler에 검증 데이터 기반에서 epoch레벨로 계산된 loss를 입력해줌.\n",
    "        if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            self.scheduler.step(running_avg_loss)\n",
    "            self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "\n",
    "        return running_avg_loss, accuracy\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate_epoch(epoch)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n",
    "                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\",\n",
    "                  f\", Current lr:{self.current_lr:.6f}\")\n",
    "            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(self.current_lr)\n",
    "\n",
    "            # if modelcheckpoint is set as an argument, call save() method based on moitor value\n",
    "            if self.checkpoint_cb:\n",
    "                if self.checkpoint_cb.monitor=='val_loss':    \n",
    "                    self.checkpoint_cb.save(self.model, epoch, val_loss)\n",
    "                elif self.checkpoint_cb.monitor == 'val_acc':\n",
    "                    self.checkpoint_cb.save(self.model, epoch, val_acc)\n",
    "                                \n",
    "        return history\n",
    "\n",
    "    # Save the model as a file when performance is improved over previous epochs. \n",
    "    # def save_checkpoint(self, epoch, val_loss):\n",
    "    #     checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch+1}_loss_{val_loss:.4f}.pt')\n",
    "    #     torch.save(self.model.state_dict(), checkpoint_path)\n",
    "    #     print(f\"Saved model checkpoint at {checkpoint_path}\")\n",
    "\n",
    "    \n",
    "    def get_trained_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95eda780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:16:39.989755Z",
     "iopub.status.busy": "2025-08-31T20:16:39.989061Z",
     "iopub.status.idle": "2025-08-31T20:19:24.034562Z",
     "shell.execute_reply": "2025-08-31T20:19:24.033649Z"
    },
    "papermill": {
     "duration": 164.088013,
     "end_time": "2025-08-31T20:19:24.035824",
     "exception": false,
     "start_time": "2025-08-31T20:16:39.947811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.55it/s, Loss=0.293, Accuracy=0.885]\n",
      "Epoch 1 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s, Loss=0.397, Accuracy=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2931 Train Accuracy: 0.8846 , Val Loss: 0.3974 Val Accuracy: 0.8371 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_1_val_loss_0.3974.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.266, Accuracy=0.888]\n",
      "Epoch 2 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.06it/s, Loss=0.177, Accuracy=0.932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.2663 Train Accuracy: 0.8876 , Val Loss: 0.1767 Val Accuracy: 0.9315 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_2_val_loss_0.1767.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.166, Accuracy=0.938]\n",
      "Epoch 3 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.61it/s, Loss=0.214, Accuracy=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.1662 Train Accuracy: 0.9380 , Val Loss: 0.2143 Val Accuracy: 0.9135 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.138, Accuracy=0.952]\n",
      "Epoch 4 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.77it/s, Loss=0.231, Accuracy=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.1379 Train Accuracy: 0.9515 , Val Loss: 0.2314 Val Accuracy: 0.9055 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.125, Accuracy=0.954]\n",
      "Epoch 5 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.28it/s, Loss=0.227, Accuracy=0.925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.1250 Train Accuracy: 0.9535 , Val Loss: 0.2270 Val Accuracy: 0.9245 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.119, Accuracy=0.958]\n",
      "Epoch 6 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.64it/s, Loss=0.176, Accuracy=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.1192 Train Accuracy: 0.9575 , Val Loss: 0.1760 Val Accuracy: 0.9300 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_6_val_loss_0.1760.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.137, Accuracy=0.954]\n",
      "Epoch 7 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.60it/s, Loss=0.17, Accuracy=0.935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.1371 Train Accuracy: 0.9540 , Val Loss: 0.1696 Val Accuracy: 0.9350 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_7_val_loss_0.1696.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.318, Accuracy=0.867]\n",
      "Epoch 8 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.40it/s, Loss=0.238, Accuracy=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.3178 Train Accuracy: 0.8671 , Val Loss: 0.2379 Val Accuracy: 0.9045 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.158, Accuracy=0.951]\n",
      "Epoch 9 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.42it/s, Loss=0.204, Accuracy=0.923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.1579 Train Accuracy: 0.9505 , Val Loss: 0.2039 Val Accuracy: 0.9225 , Current lr:0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.164, Accuracy=0.938]\n",
      "Epoch 10 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.64it/s, Loss=0.158, Accuracy=0.939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.1644 Train Accuracy: 0.9380 , Val Loss: 0.1584 Val Accuracy: 0.9385 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_10_val_loss_0.1584.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = create_resnet_model('resnet50', num_classes=2, weights='DEFAULT')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=0.001) # model.fc.parameters(), weight_decay=0.9\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(\n",
    "            optimizer=optimizer, mode='min', factor=0.5, patience=5, threshold=0.01, min_lr=0.00001)\n",
    "\n",
    "# ModelCheckpoint\n",
    "checkpoint_cb = ModelCheckpoint('checkpoints', monitor='val_loss', mode='min', save_interval=1, verbose=1)\n",
    "# checkpoint_cb = ModelCheckpoint('checkpoints', monitor='val_acc', mode='max', save_interval=1, verbose=1)\n",
    "\n",
    "trainer = Trainer_02(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                  train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, \n",
    "                  checkpoint_cb = checkpoint_cb,\n",
    "                  device=device)\n",
    "# 학습 및 평가. \n",
    "history = trainer.fit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bcc1de",
   "metadata": {
    "papermill": {
     "duration": 0.086136,
     "end_time": "2025-08-31T20:19:24.206559",
     "exception": false,
     "start_time": "2025-08-31T20:19:24.120423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create EarlyStopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d527ad0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:19:24.375881Z",
     "iopub.status.busy": "2025-08-31T20:19:24.375529Z",
     "iopub.status.idle": "2025-08-31T20:19:24.381713Z",
     "shell.execute_reply": "2025-08-31T20:19:24.381185Z"
    },
    "papermill": {
     "duration": 0.092242,
     "end_time": "2025-08-31T20:19:24.382848",
     "exception": false,
     "start_time": "2025-08-31T20:19:24.290606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, monitor='val_loss', mode='min', early_patience=5, verbose=1):\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.early_patience = early_patience\n",
    "        self.verbose = verbose\n",
    "        self.best_value = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def is_improvement(self, value):\n",
    "        if self.mode == 'min':\n",
    "            return value < self.best_value\n",
    "        else:\n",
    "            return value > self.best_value\n",
    "\n",
    "    def check_early_stop(self, value):\n",
    "        is_early_stopped = False\n",
    "        \n",
    "        if self.is_improvement(value):\n",
    "            self.best_value = value\n",
    "            self.counter = 0\n",
    "            is_early_stopped =False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.early_patience}\")\n",
    "            if self.counter >= self.early_patience:\n",
    "                is_early_stopped = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping happens and train stops\")\n",
    "        \n",
    "        return is_early_stopped\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71996c8a",
   "metadata": {
    "papermill": {
     "duration": 0.083988,
     "end_time": "2025-08-31T20:19:24.550516",
     "exception": false,
     "start_time": "2025-08-31T20:19:24.466528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Class with EarlyStopping and ModelCheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2585cbdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:19:24.720223Z",
     "iopub.status.busy": "2025-08-31T20:19:24.719715Z",
     "iopub.status.idle": "2025-08-31T20:19:24.736852Z",
     "shell.execute_reply": "2025-08-31T20:19:24.736103Z"
    },
    "papermill": {
     "duration": 0.103408,
     "end_time": "2025-08-31T20:19:24.738079",
     "exception": false,
     "start_time": "2025-08-31T20:19:24.634671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, scheduler=None, \n",
    "                 callbacks=None, device=None):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        # scheduler added\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        # add current learning rate variable\n",
    "        self.current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        # receive checkpoint and early stopping classes as a list\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "\n",
    "        # calculate running average loss\n",
    "        accu_loss = 0.0\n",
    "        running_avg_loss = 0.0\n",
    "        # accuracy, total count, and cumulative correct count for accuracy calculation\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        # visualize training loop progress in real-time with tqdm\n",
    "        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "                # must use to(self.device), not to(device)\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                # forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # calculate cumulative loss for each batch and running average loss\n",
    "                accu_loss += loss.item()\n",
    "                running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                # calculate accuracy metric\n",
    "                # calculate number of matches between predicted class and targets\n",
    "                num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                # calculate accuracy from cumulative total count and cumulative correct count\n",
    "                num_total += inputs.shape[0]\n",
    "                accu_num_correct += num_correct\n",
    "                accuracy = accu_num_correct / num_total\n",
    "\n",
    "                # update tqdm progress bar with progress, running average loss, and accuracy\n",
    "                progress_bar.update(1)\n",
    "                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # update every 20 batches or at the last batch\n",
    "                    progress_bar.set_postfix({\"Loss\": running_avg_loss,\n",
    "                                              \"Accuracy\": accuracy})\n",
    "\n",
    "        if (self.scheduler is not None) and (not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)):\n",
    "            self.scheduler.step()\n",
    "            self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "        \n",
    "        return running_avg_loss, accuracy\n",
    "\n",
    "    def validate_epoch(self, epoch):\n",
    "        if not self.val_loader:\n",
    "            return None\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # calculate running average loss\n",
    "        accu_loss = 0\n",
    "        running_avg_loss = 0\n",
    "        # accuracy, total count, and cumulative correct count for accuracy calculation\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "\n",
    "                    outputs = self.model(inputs)\n",
    "\n",
    "                    loss = self.loss_fn(outputs, targets)\n",
    "                    # calculate cumulative loss for each batch and running average loss\n",
    "                    accu_loss += loss.item()\n",
    "                    running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                    # calculate accuracy metric\n",
    "                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                    # calculate accuracy from cumulative total count and cumulative correct count\n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    accuracy = accu_num_correct / num_total\n",
    "                    \n",
    "                    # update tqdm progress bar with progress, running average loss, and accuracy\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # update every 20 batches or at the last batch\n",
    "                        progress_bar.set_postfix({\"Loss\": running_avg_loss,\n",
    "                                                  \"Accuracy\": accuracy})\n",
    "        # input validation loss to scheduler at the epoch level\n",
    "        if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            self.scheduler.step(running_avg_loss)\n",
    "            self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "\n",
    "        return running_avg_loss, accuracy\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        # create history dict to record training/validation results each epoch, including learning rate\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate_epoch(epoch)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n",
    "                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\",\n",
    "                  f\", Current lr:{self.current_lr:.6f}\")\n",
    "            # record training/validation results each epoch\n",
    "            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(self.current_lr)\n",
    "\n",
    "            # if callbacks are provided as arguments, execute them; if early stop triggered, break loop\n",
    "            if self.callbacks:\n",
    "                is_epoch_loop_break = self._execute_callbacks(self.callbacks, self.model, epoch, val_loss, val_acc)\n",
    "                if is_epoch_loop_break:\n",
    "                    break\n",
    "                                \n",
    "        return history\n",
    "\n",
    "    # execute ModelCheckpoint and EarlyStopping from callbacks list\n",
    "    # return is_early_stopped if EarlyStopping decides to stop\n",
    "    def _execute_callbacks(self, callbacks, model, epoch, val_loss, val_acc):\n",
    "        is_early_stopped = False\n",
    "        \n",
    "        for callback in self.callbacks:\n",
    "            if isinstance(callback, ModelCheckpoint):\n",
    "                if callback.monitor == 'val_loss':    \n",
    "                    callback.save(model, epoch, val_loss)\n",
    "                elif callback.monitor == 'val_acc':\n",
    "                    callback.save(model, epoch, val_acc)\n",
    "            if isinstance(callback, EarlyStopping):\n",
    "                if callback.monitor == 'val_loss':\n",
    "                    is_early_stopped = callback.check_early_stop(val_loss)\n",
    "                if callback.monitor == 'val_acc':\n",
    "                    is_early_stopped = callback.check_early_stop(val_acc)\n",
    "                \n",
    "        return is_early_stopped\n",
    "\n",
    "    # return trained model\n",
    "    def get_trained_model(self):\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b6737d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:19:24.907508Z",
     "iopub.status.busy": "2025-08-31T20:19:24.906865Z",
     "iopub.status.idle": "2025-08-31T20:23:30.136534Z",
     "shell.execute_reply": "2025-08-31T20:23:30.135436Z"
    },
    "papermill": {
     "duration": 245.315072,
     "end_time": "2025-08-31T20:23:30.137935",
     "exception": false,
     "start_time": "2025-08-31T20:19:24.822863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.325, Accuracy=0.872]\n",
      "Epoch 1 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.44it/s, Loss=0.281, Accuracy=0.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.3248 Train Accuracy: 0.8716 , Val Loss: 0.2812 Val Accuracy: 0.8711 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_1_val_acc_0.8711.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.227, Accuracy=0.899]\n",
      "Epoch 2 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.52it/s, Loss=0.295, Accuracy=0.888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 0.2270 Train Accuracy: 0.8991 , Val Loss: 0.2951 Val Accuracy: 0.8876 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_2_val_acc_0.8876.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.60it/s, Loss=0.167, Accuracy=0.937]\n",
      "Epoch 3 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.45it/s, Loss=0.288, Accuracy=0.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.1666 Train Accuracy: 0.9365 , Val Loss: 0.2884 Val Accuracy: 0.8896 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_3_val_acc_0.8896.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.165, Accuracy=0.939]\n",
      "Epoch 4 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.34it/s, Loss=0.341, Accuracy=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.1646 Train Accuracy: 0.9385 , Val Loss: 0.3405 Val Accuracy: 0.8861 , Current lr:0.001000\n",
      "EarlyStopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.182, Accuracy=0.936]\n",
      "Epoch 5 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.66it/s, Loss=0.337, Accuracy=0.888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.1818 Train Accuracy: 0.9355 , Val Loss: 0.3374 Val Accuracy: 0.8876 , Current lr:0.001000\n",
      "EarlyStopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.59it/s, Loss=0.199, Accuracy=0.923]\n",
      "Epoch 6 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.59it/s, Loss=0.333, Accuracy=0.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.1988 Train Accuracy: 0.9225 , Val Loss: 0.3330 Val Accuracy: 0.9100 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_6_val_acc_0.9100.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.139, Accuracy=0.944]\n",
      "Epoch 7 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.69it/s, Loss=0.495, Accuracy=0.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.1393 Train Accuracy: 0.9435 , Val Loss: 0.4952 Val Accuracy: 0.8386 , Current lr:0.001000\n",
      "EarlyStopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.108, Accuracy=0.964]\n",
      "Epoch 8 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.55it/s, Loss=0.31, Accuracy=0.923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 0.1082 Train Accuracy: 0.9635 , Val Loss: 0.3096 Val Accuracy: 0.9230 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_8_val_acc_0.9230.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.0603, Accuracy=0.98]\n",
      "Epoch 9 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.51it/s, Loss=0.34, Accuracy=0.915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 0.0603 Train Accuracy: 0.9795 , Val Loss: 0.3396 Val Accuracy: 0.9145 , Current lr:0.001000\n",
      "EarlyStopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.59it/s, Loss=0.0909, Accuracy=0.969]\n",
      "Epoch 10 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.57it/s, Loss=0.22, Accuracy=0.927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 0.0909 Train Accuracy: 0.9685 , Val Loss: 0.2205 Val Accuracy: 0.9270 , Current lr:0.001000\n",
      "Saved model checkpoint at checkpoints/checkpoint_epoch_10_val_acc_0.9270.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.102, Accuracy=0.969]\n",
      "Epoch 11 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.54it/s, Loss=0.254, Accuracy=0.925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 0.1021 Train Accuracy: 0.9690 , Val Loss: 0.2541 Val Accuracy: 0.9245 , Current lr:0.001000\n",
      "EarlyStopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.0972, Accuracy=0.965]\n",
      "Epoch 12 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.57it/s, Loss=1.36, Accuracy=0.842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 0.0972 Train Accuracy: 0.9650 , Val Loss: 1.3644 Val Accuracy: 0.8421 , Current lr:0.001000\n",
      "EarlyStopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.58it/s, Loss=0.0822, Accuracy=0.972]\n",
      "Epoch 13 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.59it/s, Loss=0.239, Accuracy=0.924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 0.0822 Train Accuracy: 0.9720 , Val Loss: 0.2391 Val Accuracy: 0.9240 , Current lr:0.001000\n",
      "EarlyStopping counter: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.56it/s, Loss=0.0414, Accuracy=0.987]\n",
      "Epoch 14 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.56it/s, Loss=0.219, Accuracy=0.922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 0.0414 Train Accuracy: 0.9865 , Val Loss: 0.2193 Val Accuracy: 0.9220 , Current lr:0.001000\n",
      "EarlyStopping counter: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Training..]: 100%|██████████| 126/126 [00:11<00:00, 10.57it/s, Loss=0.0694, Accuracy=0.979]\n",
      "Epoch 15 [Validating]: 100%|██████████| 63/63 [00:04<00:00, 14.57it/s, Loss=0.334, Accuracy=0.881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 0.0694 Train Accuracy: 0.9785 , Val Loss: 0.3344 Val Accuracy: 0.8806 , Current lr:0.001000\n",
      "EarlyStopping counter: 5/5\n",
      "Early stopping happens and train stops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = create_resnet_model('resnet50', num_classes=2, weights='DEFAULT')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=0.001) # model.fc.parameters(), weight_decay=0.9\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "  \n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#             optimizer=optimizer, mode='min', factor=0.5, patience=5, threshold=0.01, min_lr=0.00001)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint('checkpoints', monitor='val_acc', mode='max', save_interval=1, verbose=1)\n",
    "#checkpoint_cb = ModelCheckpoint('checkpoints', monitor='val_loss', mode='min', save_interval=1, verbose=1)\n",
    "earlystop_cb = EarlyStopping(monitor='val_acc', mode='max', early_patience=5, verbose=1)\n",
    "#earlystop_cb = EarlyStopping(monitor='val_loss', mode='min', early_patience=5, verbose=1)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint_cb, earlystop_cb] # None\n",
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                  train_loader=tr_loader, val_loader=val_loader, scheduler=None, \n",
    "                  callbacks=callbacks,\n",
    "                  device=device)\n",
    "history = trainer.fit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5e00bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:23:30.472749Z",
     "iopub.status.busy": "2025-08-31T20:23:30.472420Z",
     "iopub.status.idle": "2025-08-31T20:23:34.799010Z",
     "shell.execute_reply": "2025-08-31T20:23:34.797974Z"
    },
    "papermill": {
     "duration": 4.496974,
     "end_time": "2025-08-31T20:23:34.800164",
     "exception": false,
     "start_time": "2025-08-31T20:23:30.303190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Evaluating]: 100%|██████████| 64/64 [00:04<00:00, 14.84it/s, Accuracy=0.893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset evaluation:0.8932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "558a44ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:23:35.104622Z",
     "iopub.status.busy": "2025-08-31T20:23:35.104305Z",
     "iopub.status.idle": "2025-08-31T20:23:35.615416Z",
     "shell.execute_reply": "2025-08-31T20:23:35.614632Z"
    },
    "papermill": {
     "duration": 0.664447,
     "end_time": "2025-08-31T20:23:35.616735",
     "exception": false,
     "start_time": "2025-08-31T20:23:34.952288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /kaggle/working/checkpoints/checkpoint_epoch_5_loss_0.1924.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_ckpt = max(checkpoint_files, key=extract_epoch_num)\n",
    "print(\"Loading checkpoint:\", latest_ckpt)\n",
    "\n",
    "# Load state_dict from the latest checkpoint\n",
    "state_dict = torch.load(latest_ckpt, weights_only=True)\n",
    "\n",
    "# Create a model without pretrained weights\n",
    "new_model = create_resnet_model('resnet50', num_classes=2, weights=None)\n",
    "\n",
    "# Load the parameters from checkpoint into the model\n",
    "new_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2afb77d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:23:35.973956Z",
     "iopub.status.busy": "2025-08-31T20:23:35.973351Z",
     "iopub.status.idle": "2025-08-31T20:23:40.348065Z",
     "shell.execute_reply": "2025-08-31T20:23:40.347058Z"
    },
    "papermill": {
     "duration": 4.528282,
     "end_time": "2025-08-31T20:23:40.349396",
     "exception": false,
     "start_time": "2025-08-31T20:23:35.821114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Evaluating]: 100%|██████████| 64/64 [00:04<00:00, 14.76it/s, Accuracy=0.913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset evaluation:0.9135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model=new_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 23777,
     "sourceId": 30378,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 598.644317,
   "end_time": "2025-08-31T20:23:43.238068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-31T20:13:44.593751",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
