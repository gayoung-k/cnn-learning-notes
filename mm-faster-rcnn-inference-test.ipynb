{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n### MMDetection Installation\n\n* In the lecture video, `mmcv` was installed using `pip install mmcv-full` (this took about 10 minutes).\n* In the practice code, it was changed to `pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html` (installation took about 12 seconds, as of September 2022).\n* As of April 6, 2023, `mmdetection` was upgraded to version 3.0. Since the practice code is based on `mmdetection` 2.x, you need to install the 2.x source code.\n* In September 2024, Colab’s NumPy version was upgraded to 1.24, which caused some code execution errors. Downgrading to NumPy 1.23 fixed the issue.\n* On January 17, 2025, Colab upgraded Python from 3.10 to 3.11, along with PyTorch 2.0 and TorchVision 0.15. Accordingly, `mmcv` installation changed to:\n!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n\n* On August 25, 2025, Colab upgraded Python again, from 3.11 to 3.12, and `mmcv-full` could no longer be installed properly.\n* Because of this, the practice environment was moved from Colab to Kaggle. Kaggle still uses Python 3.11.\n* In Colab, the working directory was based on `/content`. In Kaggle, it is `/kaggle/working`, but the practice code was adjusted to use the current directory (`.`) automatically.\n* On August 25, 2025, due to SSL issues with `download.openmmlab.com`, the `--trusted-host` option had to be added to `pip install`, and the `--no-check-certificate` option added to `wget`.","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# downgrade pytorch version to 2.0\n!pip install torch==2.0.0 torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mmcv-full --trusted-host download.openmmlab.com -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n!cd mmdetection; python setup.py install","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy==1.23","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# You must restart the kernel before running the code below.\nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performing Inference Using a Faster R-CNN Pretrained Model Based on the MS-COCO Dataset\n\n* Download the Faster R-CNN pretrained model\n* Set the config file for Faster R-CNN\n* Create the inference model and apply inference\n* Due to SSL issues with the `download.openmmlab.com` site, add the `--no-check-certificate` option to `wget`.\n","metadata":{}},{"cell_type":"code","source":"!cd mmdetection; mkdir checkpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget --no-check-certificate -O /kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lia /kaggle/working/mmdetection/checkpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_file = '/kaggle/working/mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = '/kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Detector model based on the config file and pretrained model.\nfrom mmdet.apis import init_detector, inference_detector\n\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In mmdetection, when a relative path is provided as an argument, \n# it is always interpreted relative to the mmdetection directory.\n%cd mmdetection\n\nfrom mmdet.apis import init_detector, inference_detector\n\n# Pass the config and checkpoint as arguments to init_detector().\nmodel = init_detector(\n    config='configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py',\n    checkpoint='checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimg = '/kaggle/working/mmdetection/demo/demo.jpg'\n\nimg_arr  = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(12, 12))\nplt.imshow(img_arr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = '/kaggle/working/mmdetection/demo/demo.jpg'\n# The argument for inference_detector can be a string (file path), \n# a single ndarray, or a list of ndarrays.\nresults = inference_detector(model, img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(results), len(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results is a list containing 80 arrays, one for each COCO class_id from 0 to 79.\n# Each array consists of 5 values (coordinates and confidence score for the class). \n# If multiple detections exist for a class, multiple arrays are created.  \n# The coordinates follow the format: top-left (xmin, ymin) and bottom-right (xmax, ymax).  \n# The shape of each array is (number of detected objects, 5 (coordinates + confidence)).\nresults\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results[0].shape, results[1].shape, results[2].shape, results[3].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import show_result_pyplot\n# Apply the inference results to the original image to generate a new image (with bounding boxes drawn).\n# By default, only objects with a score threshold of 0.3 or higher are visualized.  \n# show_result_pyplot internally calls model.show_result().\nshow_result_pyplot(model, img, results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checking the Model’s Config Settings\n","metadata":{}},{"cell_type":"code","source":"model.__dict__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(model.cfg)\nprint(model.cfg.pretty_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### When passing an array to `inference_detector()`, the original array must be provided in BGR format (RGB conversion is handled internally, so input should be BGR).\n\n","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# must be provided in BGR format\nimg_arr = cv2.imread('/kaggle/working/mmdetection/demo/demo.jpg')\nresults = inference_detector(model, img_arr)\n\nshow_result_pyplot(model, img_arr, results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Inference Results as an Image Without Using `show_result_pyplot()`\n\n* Create a `get_detected_img()` function that takes the model and image array as input, detects objects in the image, and draws bounding boxes.\n* COCO class mapping is applied sequentially starting from 0.\n* If an array in `results` is empty, it means no object was detected for the class corresponding to that list index (class id).\n* Detections with low score thresholds are excluded.\n","metadata":{}},{"cell_type":"code","source":"labels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }\n\nlabels_to_names = {1:'person',2:'bicycle',3:'car',4:'motorcycle',5:'airplane',6:'bus',7:'train',8:'truck',9:'boat',10:'traffic light',\n                    11:'fire hydrant',12:'street sign',13:'stop sign',14:'parking meter',15:'bench',16:'bird',17:'cat',18:'dog',19:'horse',20:'sheep',\n                    21:'cow',22:'elephant',23:'bear',24:'zebra',25:'giraffe',26:'hat',27:'backpack',28:'umbrella',29:'shoe',30:'eye glasses',\n                    31:'handbag',32:'tie',33:'suitcase',34:'frisbee',35:'skis',36:'snowboard',37:'sports ball',38:'kite',39:'baseball bat',40:'baseball glove',\n                    41:'skateboard',42:'surfboard',43:'tennis racket',44:'bottle',45:'plate',46:'wine glass',47:'cup',48:'fork',49:'knife',50:'spoon',\n                    51:'bowl',52:'banana',53:'apple',54:'sandwich',55:'orange',56:'broccoli',57:'carrot',58:'hot dog',59:'pizza',60:'donut',\n                    61:'cake',62:'chair',63:'couch',64:'potted plant',65:'bed',66:'mirror',67:'dining table',68:'window',69:'desk',70:'toilet',\n                    71:'door',72:'tv',73:'laptop',74:'mouse',75:'remote',76:'keyboard',77:'cell phone',78:'microwave',79:'oven',80:'toaster',\n                    81:'sink',82:'refrigerator',83:'blender',84:'book',85:'clock',86:'vase',87:'scissors',88:'teddy bear',89:'hair drier',90:'toothbrush',\n                    91:'hair brush'}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# example of np.where\narr1 = np.array([[3.75348572e+02, 1.19171005e+02, 3.81950867e+02, 1.34460617e+02,\n         1.35454759e-01],\n        [5.32362000e+02, 1.09554726e+02, 5.40526550e+02, 1.25222633e+02,\n         8.88786465e-01],\n        [3.61124298e+02, 1.09049202e+02, 3.68625610e+02, 1.22483063e+02,\n         7.20717013e-02]], dtype=np.float32)\nprint(arr1.shape)\n\narr1_filtered = arr1[np.where(arr1[:, 4] > 0.1)]\nprint('### arr1_filtered:', arr1_filtered, arr1_filtered.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.where(arr1[:, 4] > 0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a visualization function for inference that takes the model, \n# the original image array, and the confidence score threshold for filtering as arguments.\ndef get_detected_img(model, img_array, score_threshold=0.3, is_print=True):\n  # Copy the input image array.\n  draw_img = img_array.copy()\n  bbox_color = (0, 255, 0)  # green\n  text_color = (0, 0, 255)  # red\n\n  # Perform inference detection using the model and image array as inputs,\n  # and store the results in 'results'.\n  # 'results' is a list containing 80 two-dimensional arrays (shape=(number of objects, 5)).\n  results = inference_detector(model, img_array)\n\n  # Iterate over the results list, which contains 80 arrays,\n  # and extract each 2D array to visualize objects on the image.\n  # The index in the results list corresponds directly to the COCO-mapped class id.\n  # Each 2D array contains object coordinates and class confidence scores.\n  for result_ind, result in enumerate(results):\n    # If the row size of the 2D array is 0, it means there are no detections for that class id.\n    continue\n\n    # In the 2D array, the 5th column represents the confidence score.\n    # Exclude any detections with scores lower than the threshold passed as a function argument.\n    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n\n    # Each 2D array may contain multiple detected objects for the class.\n    # Iterate through the rows to extract the coordinates of each detected object.\n    for i in range(len(result_filtered)):\n      # Extract top-left and bottom-right coordinates.\n      left = int(result_filtered[i, 0])\n      top = int(result_filtered[i, 1])\n      right = int(result_filtered[i, 2])\n      bottom = int(result_filtered[i, 3])\n      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n      if is_print:\n        print(caption)\n\n  return draw_img\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_arr = cv2.imread('/kaggle/working/mmdetection/demo/demo.jpg')\ndetected_img = get_detected_img(model, img_arr, score_threshold=0.3, is_print=True)\n# The input image for detection is in BGR format. \n# Convert it to RGB for the final output.\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -O /kaggle/working/data/beatles01.jpg https://raw.githubusercontent.com/gayoung-k/object-detection-learning-notes/image/beatles01.jpg\n!ls -lia /kaggle/working/data/beatles01.jpg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_arr = cv2.imread('/kaggle/working/data/beatles01.jpg')\ndetected_img = get_detected_img(model, img_arr,  score_threshold=0.5, is_print=True)\n\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performing Video Inference\n\n* When running video inference with `mmdetection`’s `video_demo.py`, image processing takes relatively longer.\n* Modify and apply the image processing logic.\n* Change the Colab path `/content` in the lecture video to `/kaggle/working`.\n","metadata":{}},{"cell_type":"code","source":"!wget -O /kaggle/working/data/John_Wick_small.mp4 https://github.com/gayoung-k/object-detection-learning-notes/video/John_Wick_small.mp4?raw=true","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import init_detector, inference_detector\nimport mmcv\n\nconfig_file = '/kaggle/working/mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = '/kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport cv2\n\nvideo_reader = mmcv.VideoReader('/kaggle/working/data/John_Wick_small.mp4')\nvideo_writer = None\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nvideo_writer = cv2.VideoWriter('/kaggle/working/data/John_Wick_small_out1.mp4', fourcc, video_reader.fps,(video_reader.width, video_reader.height))\nfor frame in mmcv.track_iter_progress(video_reader):\n  result = inference_detector(model, frame)\n  frame = model.show_result(frame, result, score_thr=0.4)\n\n  video_writer.write(frame)\n\nif video_writer:\n        video_writer.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Perform Video Inference by Applying a Customized Frame Processing Logic\n\n* Reuse the previously implemented `get_detected_img()` function as is.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nlabels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }\n\n# Create a visualization function for inference that takes the model, \n# the original image array, and the confidence score threshold for filtering as arguments.\ndef get_detected_img(model, img_array, score_threshold=0.3, is_print=True):\n  # Copy the input image array.\n  draw_img = img_array.copy()\n  bbox_color = (0, 255, 0)  # green\n  text_color = (0, 0, 255)  # red\n\n  # Perform inference detection using the model and image array as inputs,\n  # and store the results in 'results'.\n  # 'results' is a list containing 80 two-dimensional arrays (shape=(number of objects, 5)).\n  results = inference_detector(model, img_array)\n\n  # Iterate over the results list, which contains 80 arrays,\n  # and extract each 2D array to visualize objects on the image.\n  # The index in the results list corresponds directly to the COCO-mapped class id.\n  # Each 2D array contains object coordinates and class confidence scores.\n  for result_ind, result in enumerate(results):\n    # If the row size of the 2D array is 0, it means there are no detections for that class id.\n    continue\n\n    # In the 2D array, the 5th column represents the confidence score.\n    # Exclude any detections with scores lower than the threshold passed as a function argument.\n    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n\n    # Each 2D array may contain multiple detected objects for the class.\n    # Iterate through the rows to extract the coordinates of each detected object.\n    for i in range(len(result_filtered)):\n      # Extract top-left and bottom-right coordinates.\n      left = int(result_filtered[i, 0])\n      top = int(result_filtered[i, 1])\n      right = int(result_filtered[i, 2])\n      bottom = int(result_filtered[i, 3])\n      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n      if is_print:\n        print(caption)\n\n  return draw_img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef do_detected_video(model, input_path, output_path, score_threshold, do_print=True):\n\n    cap = cv2.VideoCapture(input_path)\n\n    codec = cv2.VideoWriter_fourcc(*'XVID')\n\n    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n\n    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n\n    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print('Total number of frames:', frame_cnt)\n    btime = time.time()\n    while True:\n        hasFrame, img_frame = cap.read()\n        if not hasFrame:\n            print('No more frames to process.')\n            break\n        stime = time.time()\n        img_frame = get_detected_img(model, img_frame, score_threshold=score_threshold, is_print=False)\n        if do_print:\n            print('Detection time per frame:', round(time.time() - stime, 4))\n        vid_writer.write(img_frame)\n    # end of while loop\n\n    vid_writer.release()\n    cap.release()\n\n    print('Total detection processing time:', round(time.time() - btime, 4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"do_detected_video(model, '/kaggle/working/data/John_Wick_small.mp4', '/kaggle/working/data/John_Wick_small_out2.mp4', score_threshold=0.4, do_print=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}